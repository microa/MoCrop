{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d2c5b0",
   "metadata": {},
   "source": [
    "# MoCrop Experimental Results\n",
    "\n",
    "This notebook contains the complete experimental results for MoCrop on UCF-101 dataset. All results are reproducible and correspond to the performance metrics shown in our paper.\n",
    "\n",
    "## Test Configuration Explanation\n",
    "\n",
    "- **Baseline (Normal Mode)**: Standard video action recognition without MoCrop\n",
    "- **MoCrop Efficiency Mode**: Uses 192Ã—192 input resolution with motion-guided cropping for computational efficiency\n",
    "- **MoCrop Attention Mode**: Uses 224Ã—224 input resolution with motion-guided cropping for maximum accuracy\n",
    "\n",
    "## Model Architectures Tested\n",
    "\n",
    "1. **ResNet-50**: Standard ResNet-50 backbone\n",
    "2. **MobileNet-V3-Large**: Lightweight mobile-optimized architecture  \n",
    "3. **EfficientNet-B1**: Efficient architecture with compound scaling\n",
    "4. **Swin-B**: Swin Transformer backbone\n",
    "5. **ResNet-152**: Deeper ResNet architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727f194-bbbc-484e-9cc2-2c3077b896a4",
   "metadata": {},
   "source": [
    "# MoCrop Experimental Results\n",
    "\n",
    "This notebook contains the complete experimental results for MoCrop on UCF-101 dataset. All results are reproducible and correspond to the performance metrics shown in our paper.\n",
    "\n",
    "## ResNet-50 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ed949-3fa5-42b2-8de0-fae8d32032c0",
   "metadata": {},
   "source": [
    "### Baseline (Normal Mode) - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2166fc8e-eb10-47f8-97ce-2a88a1e53786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/Res50_ucf101_i224_s3_val-normal_opt-adam_best.pth.tar\n",
      "arch           : resnet50\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : normal\n",
      "crop_ratio     : N/A (Not Applicable in normal mode)\n",
      "mv_h           : N/A (Not Applicable in normal mode)\n",
      "mv_w           : N/A (Not Applicable in normal mode)\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: resnet50, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/Res50_ucf101_i224_s3_val-normal_opt-adam_best.pth.tar\n",
      "test_unified.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.weights, map_location='cpu')\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using normal testing.\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 80.095% (3030/3783)\n",
      "Best accuracy recorded during training: 80.148%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/Res50_ucf101_i224_s3_val-normal_opt-adam_best.pth.tar \\\n",
    "--arch resnet50 \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode normal \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558fd7e-705f-44cc-8af6-68338f0f1dbb",
   "metadata": {},
   "source": [
    "### MoCrop Efficiency Mode - 192Ã—192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbaa7b72-c8ea-4d4f-b462-7afcd7aa4e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/Res50_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar\n",
      "arch           : resnet50\n",
      "input_size     : 192\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: resnet50, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/Res50_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar\n",
      "test_unified.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.weights, map_location='cpu')\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 82.448% (3119/3783)\n",
      "Best accuracy recorded during training: 82.448%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/Res50_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar \\\n",
    "--arch resnet50 \\\n",
    "--input-size 192 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5a8e2-091b-45d7-a485-397d3ccc5662",
   "metadata": {},
   "source": [
    "### MoCrop Attention Mode - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed8df3c4-fc27-4c9b-9b9a-515fd5eec40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/Res50_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar\n",
      "arch           : resnet50\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: resnet50, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/Res50_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 83.558% (3161/3783)\n",
      "Best accuracy recorded during training: 82.448%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/Res50_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar \\\n",
    "--arch resnet50 \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957257e0-ad86-4e0e-98ee-73274cb7ba22",
   "metadata": {},
   "source": [
    "## MobileNet-V3-Large Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b2932-9eef-475d-801f-594645088799",
   "metadata": {},
   "source": [
    "### Baseline (Normal Mode) - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7a9e36-92b8-4167-a24a-8dc9f8149077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/MNv3L_ucf101_i224_s3_val-normal_best.pth.tar\n",
      "arch           : mobilenet_v3_large\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : normal\n",
      "crop_ratio     : N/A (Not Applicable in normal mode)\n",
      "mv_h           : N/A (Not Applicable in normal mode)\n",
      "mv_w           : N/A (Not Applicable in normal mode)\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: mobilenet_v3_large, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/MNv3L_ucf101_i224_s3_val-normal_best.pth.tar\n",
      "test_unified.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.weights, map_location='cpu')\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using normal testing.\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 78.324% (2963/3783)\n",
      "Best accuracy recorded during training: 78.694%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/MNv3L_ucf101_i224_s3_val-normal_best.pth.tar \\\n",
    "--arch mobilenet_v3_large \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode normal \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83c53f-82f2-4f3e-915f-0f1b09070df7",
   "metadata": {},
   "source": [
    "### MoCrop Efficiency Mode - 192Ã—192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2ea681e-aea3-4d4b-bb73-e4fe1f8d430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/MNv3L_ucf101_i192_s3_val-mocrop_best.pth.tar\n",
      "arch           : mobilenet_v3_large\n",
      "input_size     : 192\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: mobilenet_v3_large, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/MNv3L_ucf101_i192_s3_val-mocrop_best.pth.tar\n",
      "test_unified.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.weights, map_location='cpu')\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 79.329% (3001/3783)\n",
      "Best accuracy recorded during training: 79.487%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/MNv3L_ucf101_i192_s3_val-mocrop_best.pth.tar \\\n",
    "--arch mobilenet_v3_large \\\n",
    "--input-size 192 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9a019-ad01-42c2-a5ee-e157f4ce705b",
   "metadata": {},
   "source": [
    "### MoCrop Attention Mode - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f674a230-c8b4-4ea8-b810-7033a9bf2e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/MNv3L_ucf101_i192_s3_val-mocrop_best.pth.tar\n",
      "arch           : mobilenet_v3_large\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: mobilenet_v3_large, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/MNv3L_ucf101_i192_s3_val-mocrop_best.pth.tar\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 80.227% (3035/3783)\n",
      "Best accuracy recorded during training: 79.487%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/MNv3L_ucf101_i192_s3_val-mocrop_best.pth.tar \\\n",
    "--arch mobilenet_v3_large \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bfd02-1a1c-47e7-aee6-fe9a3bc7cf39",
   "metadata": {},
   "source": [
    "## EfficientNet-B1 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284fd82-ca64-4137-abfc-00b55ce326fe",
   "metadata": {},
   "source": [
    "### Baseline (Normal Mode) - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84552dad-8378-4d73-84f0-9634c49da167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/EB1_ucf101_i224_s3_val-normal_opt-adam_best.pth.tar\n",
      "arch           : efficientnet_b1\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : normal\n",
      "crop_ratio     : N/A (Not Applicable in normal mode)\n",
      "mv_h           : N/A (Not Applicable in normal mode)\n",
      "mv_w           : N/A (Not Applicable in normal mode)\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: efficientnet_b1, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/EB1_ucf101_i224_s3_val-normal_opt-adam_best.pth.tar\n",
      "test_unified.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.weights, map_location='cpu')\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using normal testing.\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 82.131% (3107/3783)\n",
      "Best accuracy recorded during training: 82.395%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/EB1_ucf101_i224_s3_val-normal_opt-adam_best.pth.tar \\\n",
    "--arch efficientnet_b1 \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode normal \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f5498-3d6b-4ccb-9e9f-c80d77f53fd2",
   "metadata": {},
   "source": [
    "### MoCrop Efficiency Mode - 192Ã—192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ad318e0-01ad-4efc-a4d0-0035915fc5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/EB1_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar\n",
      "arch           : efficientnet_b1\n",
      "input_size     : 192\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: efficientnet_b1, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/EB1_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar\n",
      "test_unified.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.weights, map_location='cpu')\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 83.558% (3161/3783)\n",
      "Best accuracy recorded during training: 83.717%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/EB1_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar \\\n",
    "--arch efficientnet_b1 \\\n",
    "--input-size 192 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf2abc-6c48-4769-b17a-02daeb04cde0",
   "metadata": {},
   "source": [
    "### MoCrop Attention Mode - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b269ffe4-5f6f-4f55-bc1d-641c03fb13c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/EB1_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar\n",
      "arch           : efficientnet_b1\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: efficientnet_b1, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/EB1_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 84.457% (3195/3783)\n",
      "Best accuracy recorded during training: 83.717%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/EB1_ucf101_i192_s3_val-mocrop_opt-adam_best.pth.tar \\\n",
    "--arch efficientnet_b1 \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6cdc96-5d38-4ef6-bc88-39677e7169ee",
   "metadata": {},
   "source": [
    "## Swin-B (Swin Transformer) Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ec1bc-2590-41fe-829f-f885de66e749",
   "metadata": {},
   "source": [
    "### Baseline (Normal Mode) - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "961115dc-4220-4f77-b678-179e8f3a2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/SwinB_ucf101_i224_s3_val-normal_opt-adamw_best.pth.tar\n",
      "arch           : swin_b\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : normal\n",
      "crop_ratio     : N/A (Not Applicable in normal mode)\n",
      "mv_h           : N/A (Not Applicable in normal mode)\n",
      "mv_w           : N/A (Not Applicable in normal mode)\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: swin_b, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/SwinB_ucf101_i224_s3_val-normal_opt-adamw_best.pth.tar\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using normal testing.\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 87.338% (3304/3783)\n",
      "Best accuracy recorded during training: 87.365%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/SwinB_ucf101_i224_s3_val-normal_opt-adamw_best.pth.tar \\\n",
    "--arch swin_b \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode normal \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0b72c-493b-41d2-9225-2f78b3f547bf",
   "metadata": {},
   "source": [
    "### MoCrop Efficiency Mode - 192Ã—192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea6fe712-2df7-429f-b3a6-ea9f6d8b1a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/SwinB_ucf101_i192_s3_val-mocrop_opt-adamw_best.pth.tar\n",
      "arch           : swin_b\n",
      "input_size     : 192\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: swin_b, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/SwinB_ucf101_i192_s3_val-mocrop_opt-adamw_best.pth.tar\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 87.179% (3298/3783)\n",
      "Best accuracy recorded during training: 87.312%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/SwinB_ucf101_i192_s3_val-mocrop_opt-adamw_best.pth.tar \\\n",
    "--arch swin_b \\\n",
    "--input-size 192 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c993f-2acf-4a34-8fc1-5d8143b6bf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9f2ac2f-b471-4d97-ba3a-68bad419ec66",
   "metadata": {},
   "source": [
    "### MoCrop Attention Mode - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05830f72-d704-48c0-8239-09584dbb3ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/SwinB_ucf101_i224_s3_val-mocrop_opt-adamw_best.pth.tar\n",
      "arch           : swin_b\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: swin_b, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/SwinB_ucf101_i224_s3_val-mocrop_opt-adamw_best.pth.tar\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 87.470% (3309/3783)\n",
      "Best accuracy recorded during training: 87.602%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/SwinB_ucf101_i224_s3_val-mocrop_opt-adamw_best.pth.tar \\\n",
    "--arch swin_b \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94bbeb-e470-4970-9719-9d3ae20176d5",
   "metadata": {},
   "source": [
    "## ResNet-152 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f0012-efd6-41be-ae34-72aa520c9733",
   "metadata": {},
   "source": [
    "### MoCrop Efficiency Mode - 192Ã—192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfeb7cf-54ef-458a-844d-3a4afeddb324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/Res152_ucf101_i192_s3_val-mocrop_opt-sgd_best.pth.tar\n",
      "arch           : resnet152\n",
      "input_size     : 192\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 8\n",
      "-------------------------------------------------\n",
      "Initializing model with base: resnet152, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/Res152_ucf101_i192_s3_val-mocrop_opt-sgd_best.pth.tar\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/473\n",
      "  ...Processed batch 100/473\n",
      "  ...Processed batch 150/473\n",
      "  ...Processed batch 200/473\n",
      "  ...Processed batch 250/473\n",
      "  ...Processed batch 300/473\n",
      "  ...Processed batch 350/473\n",
      "  ...Processed batch 400/473\n",
      "  ...Processed batch 450/473\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 88.475% (3347/3783)\n",
      "Best accuracy recorded during training: 88.475%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/Res152_ucf101_i192_s3_val-mocrop_opt-sgd_best.pth.tar \\\n",
    "--arch resnet152 \\\n",
    "--input-size 192 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351c09f-9b10-4e85-8db0-4e7ea0edb25e",
   "metadata": {},
   "source": [
    "### MoCrop Attention Mode - 224Ã—224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3caeb31-9769-4276-930a-f343e7b589f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with the following configuration ---\n",
      "data_name      : ucf101\n",
      "data_root      : /home/mbin/data/ucf101/mpeg4_videos\n",
      "test_list      : /home/mbin/data/datalists/ucf101_split1_test.txt\n",
      "weights        : /home/mbin/MoCrop/Res152_ucf101_i192_s3_val-mocrop_opt-sgd_best.pth.tar\n",
      "arch           : resnet152\n",
      "input_size     : 224\n",
      "test_segments  : 8\n",
      "test_mode      : mocrop\n",
      "crop_ratio     : 0.75\n",
      "mv_h           : 6\n",
      "mv_w           : 8\n",
      "workers        : 8\n",
      "batch_size     : 16\n",
      "-------------------------------------------------\n",
      "Initializing model with base: resnet152, num_class: 101, num_segments: 8.\n",
      "Loading checkpoint from: /home/mbin/MoCrop/Res152_ucf101_i192_s3_val-mocrop_opt-sgd_best.pth.tar\n",
      "Model loaded successfully.\n",
      "\n",
      "INFO: Using MoCrop testing with crop_ratio=0.75, mv=(6, 8)\n",
      "Loaded 3783 videos from /home/mbin/data/datalists/ucf101_split1_test.txt.\n",
      "Starting testing on 3783 videos...\n",
      "  ...Processed batch 50/237\n",
      "  ...Processed batch 100/237\n",
      "  ...Processed batch 150/237\n",
      "  ...Processed batch 200/237\n",
      "\n",
      "ðŸš€ Testing completed!\n",
      "Accuracy: 89.188% (3374/3783)\n",
      "Best accuracy recorded during training: 88.475%\n"
     ]
    }
   ],
   "source": [
    "!python test_unified.py \\\n",
    "--weights /home/mbin/MoCrop/Res152_ucf101_i192_s3_val-mocrop_opt-sgd_best.pth.tar \\\n",
    "--arch resnet152 \\\n",
    "--input-size 224 \\\n",
    "--test-segments 8 \\\n",
    "--test-mode mocrop \\\n",
    "--crop-ratio 0.75 \\\n",
    "--mv-h 6 \\\n",
    "--mv-w 8 \\\n",
    "--batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13bd79-3398-4af2-8ebc-e73156e959b6",
   "metadata": {},
   "source": [
    "## Computational Efficiency Analysis\n",
    "\n",
    "### FLOPs (Floating Point Operations) Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f777b30a-eccb-4bbb-9396-3116b4f86bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Model Architecture        | Mode       | Input Size   | FLOPs (GFLOPs) \n",
      "--------------------------------------------------------------------------------\n",
      "resnet50                  | Normal     | 224x224      | 4.11           \n",
      "resnet50                  | MoCrop     | 192x192      | 3.02           \n",
      "resnet50                  | MoCrop     | 224x224      | 4.11           \n",
      "--------------------------------------------------------------------------------\n",
      "resnet152                 | Normal     | 224x224      | 11.56          \n",
      "resnet152                 | MoCrop     | 192x192      | 8.49           \n",
      "resnet152                 | MoCrop     | 224x224      | 11.56          \n",
      "--------------------------------------------------------------------------------\n",
      "mobilenet_v3_large        | Normal     | 224x224      | 0.22           \n",
      "mobilenet_v3_large        | MoCrop     | 192x192      | 0.17           \n",
      "mobilenet_v3_large        | MoCrop     | 224x224      | 0.22           \n",
      "--------------------------------------------------------------------------------\n",
      "efficientnet_b1           | Normal     | 224x224      | 0.59           \n",
      "efficientnet_b1           | MoCrop     | 192x192      | 0.43           \n",
      "efficientnet_b1           | MoCrop     | 224x224      | 0.59           \n",
      "--------------------------------------------------------------------------------\n",
      "swin_b                    | Normal     | 224x224      | 15.47          \n",
      "swin_b                    | MoCrop     | 192x192      | 12.64          \n",
      "swin_b                    | MoCrop     | 224x224      | 15.47          \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python calculate_flops_clean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de12fe-dd90-4f7c-9e61-41882383cf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
